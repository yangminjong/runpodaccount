# RunPod Serverless Configuration
name: exaone-accounting-serverless
handler_file: handler.py

# Docker configuration  
docker:
  base_image: nvidia/cuda:12.4.0-cudnn9-runtime-ubuntu22.04
  dockerfile_path: ./Dockerfile

# GPU requirements (flexible - can use any available GPU)
gpu:
  types:  # Priority order - RunPod will select available GPU
    - "NVIDIA A100"     # 40GB or 80GB variants
    - "NVIDIA RTX 6000" # 48GB VRAM
    - "NVIDIA A6000"    # 48GB VRAM
    - "NVIDIA A40"      # 48GB VRAM
    - "NVIDIA RTX 4090" # 24GB VRAM (minimum for model)
  count: 1
  min_memory: "24GB"  # Model requires ~18.6GB

# Resource limits
resources:
  cpu: 8
  memory: "32GB"
  
# Storage configuration
storage:
  type: "network_volume"  # RunPod network volume
  size: "80GB"  # Allocated storage
  mount_path: "/runpod-volume"
  region: "US-WA-1"  # US West region for low latency

# Scaling configuration
scaling:
  min_workers: 0  # Serverless - scale to 0
  max_workers: 5
  target_queue_size: 1
  scale_down_delay: 60  # seconds

# Environment variables (set in RunPod dashboard)
env_vars:
  - HF_TOKEN  # Hugging Face token for model download
  - HF_HOME=/runpod-volume/hf_cache  # Cache directory for models
  - CUDA_VISIBLE_DEVICES=0
  - PYTHONUNBUFFERED=1

# Timeout settings
timeout:
  max_execution_time: 300  # 5 minutes max per request
  idle_timeout: 60  # Scale down after 60 seconds idle

# Health check
health_check:
  enabled: true
  path: /health
  interval: 30  # seconds

# Deployment region
deployment:
  region: "US-WA-1"  # US West (Washington) data center
  
# Billing
billing:
  type: serverless  # Pay per second of GPU usage only
  storage_cost: "~$0.20/GB/month"  # Network volume storage cost